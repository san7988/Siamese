{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning with Siamese Networks\n",
    "\n",
    "This is the jupyter notebook that accompanies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All the imports are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import img_to_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Set of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class\n",
    "A simple class to manage configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"data/train_2/\"\n",
    "    testing_dir = \"data/test/\"\n",
    "    custom_test_dir = \"data/custom_test/\"\n",
    "    train_batch_size = 64\n",
    "    train_number_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_dataset = dset.ImageFolder(root=Config.custom_test_dir)\n",
    "# folder_dataset.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer1.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer2.requires_grad=False\n",
    "model.conv1.requires_grad = False\n",
    "model.bn1.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.children())[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# layer = model._modules.get('layer2')\n",
    "# model.cuda()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_name, is_path=True):\n",
    "    \n",
    "    if is_path:\n",
    "        img = Image.open(image_name)\n",
    "        t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0)).cuda()\n",
    "    else:\n",
    "        img = image_name\n",
    "        t_img = Variable(normalize(img).unsqueeze(0), dt).cuda()\n",
    "    \n",
    "    image_embedding = []\n",
    "    \n",
    "    def copy_data(m, i, o):\n",
    "        image_embedding.append(o.data)\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "    model(t_img)\n",
    "    h.remove()\n",
    "    return image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "This dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True,\n",
    "                 is_test=False,pick_similar_samples=True,\n",
    "                is_custom_test=False):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        self.is_test = is_test\n",
    "        self.pick_similar_samples = pick_similar_samples\n",
    "        self.is_custom_test = is_custom_test\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if not self.is_custom_test:\n",
    "            img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "            #we need to make sure approx 50% of images are in the same class\n",
    "            if not self.is_test:\n",
    "                should_get_same_class = random.randint(0,1)\n",
    "            else:\n",
    "                if self.pick_similar_samples : should_get_same_class = 1\n",
    "                else: should_get_same_class = 0\n",
    "\n",
    "            if should_get_same_class:\n",
    "                while True:\n",
    "                    #keep looping till the same class image is found\n",
    "                    img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                    if img0_tuple[1]==img1_tuple[1]:\n",
    "                        break\n",
    "            else:\n",
    "                while True:\n",
    "                    #keep looping till a different class image is found\n",
    "\n",
    "                    img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                    if img0_tuple[1] !=img1_tuple[1]:\n",
    "                        break\n",
    "        else:\n",
    "            img0_tuple = (self.imageFolderDataset.imgs[0])\n",
    "            img1_tuple = (self.imageFolderDataset.imgs[1])\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        #img0 = img0.convert(\"L\")\n",
    "        #img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "#             print(\"type of img0: {0}\".format(type(img0)))\n",
    "#             print(\"type of img1: {0}\".format(type(img1)))\n",
    "#         print(\"ïmg 0 shape: {0}\".format(img0.shape))\n",
    "#         img0_vec = get_vector(img0_tuple[0], is_path=True)[0]\n",
    "#         img1_vec = get_vector(img1_tuple[0], is_path=True)[0]\n",
    "#         img0_vec.squeeze_()\n",
    "#         img1_vec.squeeze_()\n",
    "#         print(\"ïmg 0 shape: {0}\".format(img0_vec.shape))\n",
    "#         , img0_vec, img1_vec \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Image Folder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 7583\n",
       "    Root Location: data/train_2/\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_dataset = dset.ImageFolder(root=Config.training_dir)\n",
    "folder_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.RandomAffine([5,7,10,13,15], translate=(5,15)),\n",
    "#transforms.RandomHorizontalFlip(p=0.5),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                      transforms.ColorJitter(brightness=0.1,contrast=0.1),\n",
    "                                                                      \n",
    "                                                                      transforms.RandomRotation([0,75]),\n",
    "                                                                      transforms.RandomAffine([0,20], translate=(0.1, 0.95), scale=(0.5,2), shear=5),\n",
    "                                                                      transforms.ToTensor(),\n",
    "                                                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        batch_size=8)\n",
    "dataiter = iter(dataloader)\n",
    "# dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising some of the data\n",
    "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n",
    "1 indiciates dissimilar, and 0 indicates similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_dataloader = DataLoader(siamese_dataset,\n",
    "#                         shuffle=True,\n",
    "#                         num_workers=0,\n",
    "#                         batch_size=8)\n",
    "# dataiter = iter(vis_dataloader)\n",
    "\n",
    "\n",
    "# example_batch = next(dataiter)\n",
    "# # print(example_batch[0].shape)\n",
    "# concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "# print(concatenated.size())\n",
    "# #imshow(torchvision.utils.make_grid(concatenated))\n",
    "# print(example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Net Definition\n",
    "We will use a standard convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.resnet_part = nn.Sequential(*list(model.children())[:-3])\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 128, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(128, 32, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(32, 16, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "\n",
    "\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16*14*14, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 5))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.resnet_part(x)\n",
    "        output = self.cnn1(output)\n",
    "#         print(\"1: {0}\".format(output.shape))\n",
    "        output = output.view(output.size()[0], -1)\n",
    "#         print(\"2: {0}\".format(output.shape))\n",
    "        output = self.fc1(output)\n",
    "#         print(\"3: {0}\".format(output.shape))\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        #print(input1)\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        batch_size=Config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork()\n",
    "net.cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# summary(net,(128,224,224),device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 1.9051463603973389\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.2419283390045166\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.1043168306350708\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.075584053993225\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.2841763496398926\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.138029932975769\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.077765941619873\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.0643932819366455\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.2205801010131836\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.1360487937927246\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.1136553287506104\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 1.1131433248519897\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.097383975982666\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.076401948928833\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.0855332612991333\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.1108978986740112\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.1231343746185303\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.0644862651824951\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.1009690761566162\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.0956401824951172\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.145287275314331\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.1522252559661865\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.0930649042129517\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.1111741065979004\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.2544488906860352\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.0415458679199219\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.0924339294433594\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.1009728908538818\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.0739672183990479\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.0859310626983643\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.1018568277359009\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.1292322874069214\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.1159694194793701\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.1604315042495728\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.174140214920044\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.0731048583984375\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.0756715536117554\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1007024049758911\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1966228485107422\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1070425510406494\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1160528659820557\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.089221715927124\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.0950448513031006\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.2059009075164795\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.0755021572113037\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1189792156219482\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.007856845855713\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1541359424591064\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0933128595352173\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0821342468261719\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.118537425994873\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0899319648742676\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0819824934005737\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0819095373153687\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0546337366104126\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.9946764707565308\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.110393762588501\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0806715488433838\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0796177387237549\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.0868139266967773\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0600485801696777\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0878651142120361\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.1303755044937134\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0959275960922241\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0487391948699951\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0876888036727905\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.1055514812469482\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0452990531921387\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0963555574417114\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0943611860275269\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.0767576694488525\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.068513035774231\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.1025493144989014\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0807448625564575\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0746186971664429\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0845229625701904\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.1195662021636963\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0403196811676025\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0578491687774658\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.1052086353302002\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0965873003005981\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.1048402786254883\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0611552000045776\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0847537517547607\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.079472541809082\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.085369348526001\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.037635087966919\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.1296722888946533\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.0603312253952026\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.06475830078125\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.110610008239746\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.0687878131866455\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.113943338394165\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.0907518863677979\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.0582996606826782\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.0694462060928345\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0631816387176514\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.1164627075195312\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0811724662780762\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0722167491912842\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.1559292078018188\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.1124904155731201\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0765483379364014\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0862327814102173\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0382590293884277\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.123746633529663\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0782647132873535\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.0454297065734863\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1359847784042358\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1307154893875122\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.0517048835754395\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1376841068267822\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.040081262588501\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1077618598937988\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.0927002429962158\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.134210228919983\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1125810146331787\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.0909568071365356\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.0765657424926758\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1043462753295898\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.0555684566497803\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.083754539489746\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.9654091596603394\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.0646164417266846\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.107245922088623\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.076194405555725\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.0312697887420654\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.0791873931884766\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.0756489038467407\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.112494707107544\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.080461025238037\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.0542008876800537\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0956496000289917\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0642871856689453\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.088248610496521\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0171480178833008\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0812067985534668\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.9794114232063293\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0800920724868774\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.084742546081543\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0797748565673828\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.0995033979415894\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.116323471069336\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.1032379865646362\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0090807676315308\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.1061630249023438\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.10395348072052\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0814011096954346\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.112918734550476\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0715690851211548\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.102865219116211\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0454955101013184\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0791637897491455\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.1179838180541992\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0557489395141602\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 1.0830975770950317\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.0783295631408691\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.0774171352386475\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.051824688911438\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.084049105644226\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.1032788753509521\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.095983624458313\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.1236460208892822\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.080914855003357\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.0943357944488525\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.1109281778335571\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.079061508178711\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.0693740844726562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 14\n",
      " Current loss 1.0630662441253662\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0411686897277832\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0784475803375244\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0459060668945312\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.1243265867233276\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.1587238311767578\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0919888019561768\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0599403381347656\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0822688341140747\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0622462034225464\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0789886713027954\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.0617486238479614\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.071565866470337\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0399187803268433\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.131812572479248\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0576460361480713\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0806816816329956\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0397751331329346\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0397779941558838\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.1200428009033203\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.101784348487854\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0977774858474731\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0458725690841675\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.0437564849853516\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0860638618469238\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0783576965332031\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0945353507995605\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0738996267318726\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.1468089818954468\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0660409927368164\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0975947380065918\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0787508487701416\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.1271827220916748\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0926413536071777\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.0643943548202515\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.1035168170928955\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.037147045135498\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.1077550649642944\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0815776586532593\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.099761962890625\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0381271839141846\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0600231885910034\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0878994464874268\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0991824865341187\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0965021848678589\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0675463676452637\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.0694115161895752\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.100658655166626\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.078623652458191\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0555998086929321\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0899864435195923\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.074181318283081\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0552477836608887\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.11592698097229\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0670883655548096\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0825284719467163\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0223777294158936\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.1297301054000854\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.124356985092163\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.0806879997253418\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.1296005249023438\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.087506651878357\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0556570291519165\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0538880825042725\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0633552074432373\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.042188048362732\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0207273960113525\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0757832527160645\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0636954307556152\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0996360778808594\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.0964382886886597\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.048468828201294\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0527119636535645\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0474889278411865\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0700656175613403\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0707149505615234\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0553375482559204\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0769119262695312\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0837414264678955\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0641534328460693\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0774240493774414\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.1112031936645508\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.0884356498718262\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.119848608970642\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.090285062789917\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0130023956298828\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0749506950378418\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0884073972702026\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.1563057899475098\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0461252927780151\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.1209584474563599\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0746079683303833\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0535929203033447\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.1204800605773926\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.0828660726547241\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.154264211654663\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0737122297286987\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0275968313217163\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0757524967193604\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0822834968566895\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0291755199432373\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0471680164337158\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0305386781692505\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.060225248336792\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.093827724456787\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.123093605041504\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.11911940574646\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.1345188617706299\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0739004611968994\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0865586996078491\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0279204845428467\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.084885597229004\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0779316425323486\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.1045341491699219\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0661985874176025\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.1544097661972046\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.065942406654358\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.1233751773834229\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0628248453140259\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0990333557128906\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0844905376434326\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.044847011566162\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.1102726459503174\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0605754852294922\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.1294270753860474\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.1311633586883545\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0995652675628662\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.1161807775497437\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0257729291915894\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0582311153411865\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0442631244659424\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.0562975406646729\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.066629409790039\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.054445743560791\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.035116195678711\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.0851740837097168\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.086729645729065\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.027707815170288\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.0243141651153564\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.1048080921173096\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.0719133615493774\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.0657447576522827\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.080289602279663\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.0833674669265747\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0456814765930176\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0878946781158447\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0726983547210693\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0351433753967285\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.05609929561615\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0720758438110352\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.1469244956970215\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0973879098892212\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0525226593017578\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0532255172729492\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0467877388000488\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.0364309549331665\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0536288022994995\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.08583664894104\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0358811616897583\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0617314577102661\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0606069564819336\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0539757013320923\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.067223310470581\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0353076457977295\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0623966455459595\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 27\n",
      " Current loss 1.0560228824615479\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0618419647216797\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.0986006259918213\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0665969848632812\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0449575185775757\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.1124377250671387\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.059473991394043\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0827158689498901\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0607162714004517\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0440253019332886\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0804423093795776\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.1127893924713135\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.087934970855713\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0819531679153442\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.069549560546875\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0782184600830078\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.1129422187805176\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0427459478378296\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0708547830581665\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.1003645658493042\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0634055137634277\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0327142477035522\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.1154354810714722\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0955748558044434\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.1054656505584717\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0709898471832275\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.0572540760040283\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.070859670639038\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.049537181854248\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0921742916107178\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.125657081604004\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0600392818450928\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0830633640289307\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.058023452758789\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.088428020477295\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0617942810058594\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0899919271469116\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0625149011611938\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0970134735107422\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0526838302612305\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0850696563720703\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.069697618484497\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0694375038146973\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.047764778137207\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.145003318786621\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0663878917694092\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.1088589429855347\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0610398054122925\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0945026874542236\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0873494148254395\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.0861074924468994\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0605616569519043\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0918124914169312\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0885961055755615\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.1163280010223389\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.1300389766693115\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0818262100219727\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0500133037567139\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0538115501403809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### if __name__ == \"__main__\":\n",
    "for epoch in range(0,Config.train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1 , label = data\n",
    "        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n",
    "#             print(\"img0: \",img0.is_cuda)\n",
    "#             print(\"img1: \",img1.is_cuda)\n",
    "#             print(\"model: \",next(net.parameters()).is_cuda)\n",
    "#         print(\"In traing loop: {0}\".format(img0.shape))\n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = net(img0,img1)\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.item())\n",
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing\n",
    "The last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"model\")\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Resize((229,229)),\n",
    "                                                                      \n",
    "                                                                      transforms.Grayscale(num_output_channels=1),\n",
    "                                                                      transforms.RandomRotation([5,75]),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False,\n",
    "                                        is_test=True,\n",
    "                                       pick_similar_samples=True)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset,num_workers=0,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "#x0,_,_ = next(dataiter)\n",
    "\n",
    "for i in range(10):\n",
    "    _x0,_x1,x0,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((_x0,_x1),0)\n",
    "    \n",
    "    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Resize((229,229)),\n",
    "                                                                      \n",
    "                                                                      transforms.Grayscale(num_output_channels=1),\n",
    "                                                                      transforms.RandomRotation([5,75]),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False,\n",
    "                                        is_test=True,\n",
    "                                       pick_similar_samples=False)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset,num_workers=0,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "#x0,_,_ = next(dataiter)\n",
    "\n",
    "for i in range(10):\n",
    "    _x0,_x1,x0,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((_x0,_x1),0)\n",
    "    \n",
    "    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.custom_test_dir)\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                      \n",
    "                                                                      transforms.Grayscale(num_output_channels=1),\n",
    "                                                                      transforms.RandomRotation([5,75]),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False,\n",
    "                                        is_test=True,\n",
    "                                       pick_similar_samples=False,\n",
    "                                       is_custom_test=True)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset,num_workers=0,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "#x0,_,_ = next(dataiter)\n",
    "\n",
    "for i in range(1):\n",
    "    _x0,_x1,x0,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((_x0,_x1),0)\n",
    "    \n",
    "    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
